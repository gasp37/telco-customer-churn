{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d478c5de20a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'teleco-customer-churn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gasp3\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    196\u001b[0m             )\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             self._do_init(\n",
      "\u001b[1;32mc:\\Users\\gasp3\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gasp3\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName='teleco-customer-churn')\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will work more as a guide to help us develop the script that will run on GCP, and since we will run it localy, I'll just grab a sample of around 1000 records, so we can do it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_table = spark.read.csv('../data/WA_Fn-UseC_-Telco-Customer-Churn.csv', header='true', inferSchema='true')\n",
    "customers_table_sample = customers_table.sample(withReplacement=False, fraction=0.15, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1102"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_table_sample.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by applying the same steps we did on the analysis notebook to treat missing values and standardize column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_table_sample = customers_table_sample.withColumnRenamed('gender', 'Gender').withColumnRenamed('tenure', 'Tenure').withColumnRenamed('customerId', 'CustomerId')\n",
    "customers_table_sample = customers_table_sample.replace(subset='TotalCharges', to_replace=' ', value='0.00')\n",
    "customers_table_sample = customers_table_sample.withColumn('TotalCharges', customers_table_sample.TotalCharges.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|CustomerId|Gender|SeniorCitizen|Partner|Dependents|Tenure|PhoneService|   MultipleLines|InternetService|     OnlineSecurity|       OnlineBackup|   DeviceProtection|        TechSupport|        StreamingTV|    StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|6713-OKOMC|Female|            0|     No|        No|    10|          No|No phone service|            DSL|                Yes|                 No|                 No|                 No|                 No|                 No|Month-to-month|              No|        Mailed check|         29.75|       301.9|   No|\n",
      "|8191-XWSZG|Female|            0|     No|        No|    52|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      One year|              No|        Mailed check|         20.65|     1022.95|   No|\n",
      "|4190-MFLUW|Female|            0|    Yes|       Yes|    10|         Yes|              No|            DSL|                 No|                 No|                Yes|                Yes|                 No|                 No|Month-to-month|              No|Credit card (auto...|          55.2|      528.35|  Yes|\n",
      "|7639-LIAYI|  Male|            0|     No|        No|    52|         Yes|             Yes|            DSL|                Yes|                 No|                 No|                Yes|                Yes|                Yes|      Two year|             Yes|Credit card (auto...|         79.75|      4217.8|   No|\n",
      "|8012-SOUDQ|Female|            1|     No|        No|    43|         Yes|             Yes|    Fiber optic|                 No|                Yes|                 No|                 No|                Yes|                 No|Month-to-month|             Yes|    Electronic check|         90.25|     3838.75|   No|\n",
      "|3957-SQXML|Female|            0|    Yes|       Yes|    34|         Yes|             Yes|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      Two year|              No|Credit card (auto...|         24.95|       894.3|   No|\n",
      "|0557-ASKVU|Female|            0|    Yes|       Yes|    18|         Yes|              No|            DSL|                 No|                 No|                Yes|                Yes|                 No|                 No|      One year|             Yes|Credit card (auto...|          54.4|       957.1|   No|\n",
      "|8627-ZYGSZ|  Male|            0|    Yes|        No|    47|         Yes|             Yes|    Fiber optic|                 No|                Yes|                 No|                 No|                 No|                 No|      One year|             Yes|    Electronic check|          78.9|     3650.35|   No|\n",
      "|1891-QRQSA|  Male|            1|    Yes|       Yes|    64|         Yes|             Yes|    Fiber optic|                Yes|                 No|                Yes|                Yes|                Yes|                Yes|      Two year|             Yes|Bank transfer (au...|         111.6|      7099.0|   No|\n",
      "|3887-PBQAO|Female|            0|    Yes|       Yes|    45|         Yes|             Yes|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      One year|             Yes|Credit card (auto...|          25.9|      1216.6|   No|\n",
      "|2796-NNUFI|Female|            0|    Yes|       Yes|    46|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      Two year|             Yes|        Mailed check|         19.95|       927.1|   No|\n",
      "|4767-HZZHQ|  Male|            0|    Yes|       Yes|    30|         Yes|              No|    Fiber optic|                 No|                Yes|                Yes|                 No|                 No|                 No|Month-to-month|              No|Bank transfer (au...|         82.05|      2570.2|   No|\n",
      "|5386-THSLQ|Female|            1|    Yes|        No|    66|          No|No phone service|            DSL|                 No|                Yes|                Yes|                 No|                Yes|                 No|      One year|              No|Bank transfer (au...|         45.55|     3027.25|   No|\n",
      "|6180-YBIQI|  Male|            0|     No|        No|     5|          No|No phone service|            DSL|                 No|                 No|                 No|                 No|                 No|                 No|Month-to-month|              No|        Mailed check|          24.3|       100.2|   No|\n",
      "|6728-DKUCO|Female|            0|    Yes|       Yes|    72|         Yes|             Yes|    Fiber optic|                Yes|                Yes|                 No|                 No|                Yes|                Yes|      One year|             Yes|    Electronic check|        104.15|     7303.05|   No|\n",
      "|2848-YXSMW|  Male|            0|    Yes|       Yes|    72|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      Two year|              No|Credit card (auto...|          19.4|     1363.25|   No|\n",
      "|0404-SWRVG|  Male|            0|     No|        No|     3|         Yes|             Yes|    Fiber optic|                 No|                 No|                 No|                 No|                 No|                 No|Month-to-month|             Yes|    Electronic check|          74.4|      229.55|  Yes|\n",
      "|3930-ZGWVE|  Male|            0|     No|        No|     1|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|Month-to-month|              No|        Mailed check|         19.75|       19.75|   No|\n",
      "|2876-GZYZC|Female|            0|     No|        No|    13|         Yes|             Yes|    Fiber optic|                 No|                 No|                 No|                 No|                 No|                Yes|Month-to-month|             Yes|    Electronic check|         85.95|     1215.65|   No|\n",
      "|6217-KDYWC|  Male|            0|     No|       Yes|    57|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      Two year|             Yes|        Mailed check|          19.6|     1170.55|   No|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_table_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CustomerId: string, Gender: string, SeniorCitizen: int, Partner: string, Dependents: string, Tenure: int, PhoneService: string, MultipleLines: string, InternetService: string, OnlineSecurity: string, OnlineBackup: string, DeviceProtection: string, TechSupport: string, StreamingTV: string, StreamingMovies: string, Contract: string, PaperlessBilling: string, PaymentMethod: string, MonthlyCharges: double, TotalCharges: double, Churn: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_table_sample.distinct()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "---\n",
    "First, we'll drop de Id column, since it doesn't present any predictive value. Then we'll convert the categorical string variables into numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_table_sample = customers_table_sample.drop('CustomerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+--------------+------------+------+-------+----------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+-----+\n",
      "|SeniorCitizen|Tenure|MonthlyCharges|TotalCharges|Gender|Partner|Dependents|PhoneService|MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|Contract|PaperlessBilling|PaymentMethod|Churn|\n",
      "+-------------+------+--------------+------------+------+-------+----------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+-----+\n",
      "|            0|    10|         29.75|       301.9|   0.0|    0.0|       0.0|         1.0|          2.0|            1.0|           1.0|         0.0|             0.0|        0.0|        1.0|            0.0|     0.0|             1.0|          3.0|  0.0|\n",
      "|            0|    52|         20.65|     1022.95|   0.0|    0.0|       0.0|         0.0|          0.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     2.0|             1.0|          3.0|  0.0|\n",
      "|            0|    10|          55.2|      528.35|   0.0|    1.0|       1.0|         0.0|          0.0|            1.0|           0.0|         0.0|             1.0|        1.0|        1.0|            0.0|     0.0|             1.0|          1.0|  1.0|\n",
      "|            0|    52|         79.75|      4217.8|   1.0|    0.0|       0.0|         0.0|          1.0|            1.0|           1.0|         0.0|             0.0|        1.0|        0.0|            1.0|     1.0|             0.0|          1.0|  0.0|\n",
      "|            1|    43|         90.25|     3838.75|   0.0|    0.0|       0.0|         0.0|          1.0|            0.0|           0.0|         1.0|             0.0|        0.0|        0.0|            0.0|     0.0|             0.0|          0.0|  0.0|\n",
      "|            0|    34|         24.95|       894.3|   0.0|    1.0|       1.0|         0.0|          1.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     1.0|             1.0|          1.0|  0.0|\n",
      "|            0|    18|          54.4|       957.1|   0.0|    1.0|       1.0|         0.0|          0.0|            1.0|           0.0|         0.0|             1.0|        1.0|        1.0|            0.0|     2.0|             0.0|          1.0|  0.0|\n",
      "|            0|    47|          78.9|     3650.35|   1.0|    1.0|       0.0|         0.0|          1.0|            0.0|           0.0|         1.0|             0.0|        0.0|        1.0|            0.0|     2.0|             0.0|          0.0|  0.0|\n",
      "|            1|    64|         111.6|      7099.0|   1.0|    1.0|       1.0|         0.0|          1.0|            0.0|           1.0|         0.0|             1.0|        1.0|        0.0|            1.0|     1.0|             0.0|          2.0|  0.0|\n",
      "|            0|    45|          25.9|      1216.6|   0.0|    1.0|       1.0|         0.0|          1.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     2.0|             0.0|          1.0|  0.0|\n",
      "|            0|    46|         19.95|       927.1|   0.0|    1.0|       1.0|         0.0|          0.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     1.0|             0.0|          3.0|  0.0|\n",
      "|            0|    30|         82.05|      2570.2|   1.0|    1.0|       1.0|         0.0|          0.0|            0.0|           0.0|         1.0|             1.0|        0.0|        1.0|            0.0|     0.0|             1.0|          2.0|  0.0|\n",
      "|            1|    66|         45.55|     3027.25|   0.0|    1.0|       0.0|         1.0|          2.0|            1.0|           0.0|         1.0|             1.0|        0.0|        0.0|            0.0|     2.0|             1.0|          2.0|  0.0|\n",
      "|            0|     5|          24.3|       100.2|   1.0|    0.0|       0.0|         1.0|          2.0|            1.0|           0.0|         0.0|             0.0|        0.0|        1.0|            0.0|     0.0|             1.0|          3.0|  0.0|\n",
      "|            0|    72|        104.15|     7303.05|   0.0|    1.0|       1.0|         0.0|          1.0|            0.0|           1.0|         1.0|             0.0|        0.0|        0.0|            1.0|     2.0|             0.0|          0.0|  0.0|\n",
      "|            0|    72|          19.4|     1363.25|   1.0|    1.0|       1.0|         0.0|          0.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     1.0|             1.0|          1.0|  0.0|\n",
      "|            0|     3|          74.4|      229.55|   1.0|    0.0|       0.0|         0.0|          1.0|            0.0|           0.0|         0.0|             0.0|        0.0|        1.0|            0.0|     0.0|             0.0|          0.0|  1.0|\n",
      "|            0|     1|         19.75|       19.75|   1.0|    0.0|       0.0|         0.0|          0.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     0.0|             1.0|          3.0|  0.0|\n",
      "|            0|    13|         85.95|     1215.65|   0.0|    0.0|       0.0|         0.0|          1.0|            0.0|           0.0|         0.0|             0.0|        0.0|        1.0|            1.0|     0.0|             0.0|          0.0|  0.0|\n",
      "|            0|    57|          19.6|     1170.55|   1.0|    0.0|       1.0|         0.0|          0.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     1.0|             0.0|          3.0|  0.0|\n",
      "+-------------+------+--------------+------------+------+-------+----------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_variables = [variable[0] for variable in customers_table_sample.dtypes if variable[1] == 'string']\n",
    "output_string_variables = [variable+'_numeric' for variable in string_variables]\n",
    "rename_columns_dic = {output_string_variables[index]:string_variables[index] for index in range(len(string_variables))}\n",
    "\n",
    "indexer_model = StringIndexer(inputCols=string_variables, outputCols=output_string_variables)\n",
    "indexer_fitted = indexer_model.fit(customers_table_sample)\n",
    "numeric_customers_table = indexer_fitted.transform(customers_table_sample)\n",
    "\n",
    "numeric_customers_table = numeric_customers_table.drop(*string_variables)\n",
    "numeric_customers_table = numeric_customers_table.withColumnsRenamed(rename_columns_dic)\n",
    "\n",
    "numeric_customers_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SeniorCitizen', 'int'),\n",
       " ('Tenure', 'int'),\n",
       " ('MonthlyCharges', 'double'),\n",
       " ('TotalCharges', 'double'),\n",
       " ('Gender', 'double'),\n",
       " ('Partner', 'double'),\n",
       " ('Dependents', 'double'),\n",
       " ('PhoneService', 'double'),\n",
       " ('MultipleLines', 'double'),\n",
       " ('InternetService', 'double'),\n",
       " ('OnlineSecurity', 'double'),\n",
       " ('OnlineBackup', 'double'),\n",
       " ('DeviceProtection', 'double'),\n",
       " ('TechSupport', 'double'),\n",
       " ('StreamingTV', 'double'),\n",
       " ('StreamingMovies', 'double'),\n",
       " ('Contract', 'double'),\n",
       " ('PaperlessBilling', 'double'),\n",
       " ('PaymentMethod', 'double'),\n",
       " ('Churn', 'double')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_customers_table.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we got all the variables set as numeric values. We will now create our first model so we can use it as a baseline. I don't expect it to be the most accurate, but after that we can dig more into other pre-processing techniques that will later on improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll just create a few functions that will help us pre-process and evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitter(dataframe, test_ratio = 0.7, seed=42):\n",
    "    pre_split_dataframe = dataframe.withColumn('train_test_index', rand(seed=seed))\n",
    "    \n",
    "    train_dataframe = pre_split_dataframe.filter(pre_split_dataframe.train_test_index <= test_ratio)\n",
    "    test_dataframe = pre_split_dataframe.filter(pre_split_dataframe.train_test_index > test_ratio)\n",
    "\n",
    "    train_dataframe = train_dataframe.drop('train_test_index')\n",
    "    test_dataframe = test_dataframe.drop('train_test_index')\n",
    "\n",
    "    print(f'Rows on train dataframe: {train_dataframe.count()}\\nRows on test dataframe: {test_dataframe.count()}')\n",
    "    return train_dataframe, test_dataframe\n",
    "\n",
    "\n",
    "def vectorize_dataframe(dataframe, label):\n",
    "    features_cols = dataframe.drop(label).columns\n",
    "\n",
    "    vecAssembler = VectorAssembler(inputCols=features_cols, outputCol='features')\n",
    "    vectorized_df = vecAssembler.transform(dataframe)\n",
    "    vectorized_df = vectorized_df.drop(*features_cols)\n",
    "\n",
    "    return vectorized_df\n",
    "\n",
    "def evaluate_model(model, dataframe):\n",
    "    prediction = model.transform(dataframe)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='Churn', metricName='f1', metricLabel=1.0)\n",
    "    f1_score = evaluator.evaluate(prediction)\n",
    "    accuracy_score = evaluator.evaluate(prediction, {evaluator.metricName:'accuracy'})\n",
    "    recall_score = evaluator.evaluate(prediction, {evaluator.metricName:'recallByLabel'})\n",
    "\n",
    "    confusion_matrix = prediction.groupBy('Churn', 'prediction').count()\n",
    "\n",
    "    return f1_score, accuracy_score, recall_score, confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can create our model, we need to do a train-test split. I won't be using RandomSplit() as it presents some unstable results. [You can read about it here](https://sergei-ivanov.medium.com/why-you-should-not-use-randomsplit-in-pyspark-to-split-data-into-train-and-test-58576d539a36). Instead, we'll create a column with random values and filter it, and then we'll vectorize those datasets so we can have them ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows on train dataframe: 815\n",
      "Rows on test dataframe: 287\n"
     ]
    }
   ],
   "source": [
    "first_train_df, first_test_df = train_test_splitter(numeric_customers_table)\n",
    "vectorized_train_df = vectorize_dataframe(first_train_df, label='Churn')\n",
    "vectorized_test_df = vectorize_dataframe(first_test_df, label='Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------+\n",
      "|Churn|features                                                                             |\n",
      "+-----+-------------------------------------------------------------------------------------+\n",
      "|0.0  |(19,[1,2,3,7,8,9,10,14,17,18],[10.0,29.75,301.9,1.0,2.0,1.0,1.0,1.0,1.0,3.0])        |\n",
      "|0.0  |[0.0,52.0,20.65,1022.95,0.0,0.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,1.0,3.0] |\n",
      "|0.0  |(19,[1,2,3,4,8,9,10,13,15,16,18],[52.0,79.75,4217.8,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|0.0  |(19,[0,1,2,3,8,11],[1.0,43.0,90.25,3838.75,1.0,1.0])                                 |\n",
      "|0.0  |[0.0,34.0,24.95,894.3,0.0,1.0,1.0,0.0,1.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0]   |\n",
      "|0.0  |(19,[1,2,3,4,5,8,11,14,16],[47.0,78.9,3650.35,1.0,1.0,1.0,1.0,1.0,2.0])              |\n",
      "|0.0  |[0.0,46.0,19.95,927.1,0.0,1.0,1.0,0.0,0.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,1.0,0.0,3.0]   |\n",
      "|0.0  |(19,[1,2,3,4,5,6,11,12,14,17,18],[30.0,82.05,2570.2,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0])|\n",
      "|0.0  |[1.0,66.0,45.55,3027.25,0.0,1.0,0.0,1.0,2.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,2.0,1.0,2.0] |\n",
      "|1.0  |(19,[1,2,3,4,8,14],[3.0,74.4,229.55,1.0,1.0,1.0])                                    |\n",
      "|0.0  |[0.0,1.0,19.75,19.75,1.0,0.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,0.0,1.0,3.0]    |\n",
      "|0.0  |(19,[1,2,3,8,14,15],[13.0,85.95,1215.65,1.0,1.0,1.0])                                |\n",
      "|1.0  |(19,[1,2,3,8,9,10,13,14,15,17],[8.0,71.15,563.65,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |\n",
      "|1.0  |(19,[1,2,3,5,8,10,14,17],[20.0,82.4,1592.35,1.0,1.0,1.0,1.0,1.0])                    |\n",
      "|0.0  |(19,[1,2,3,5,6,8,11,12,15,18],[15.0,105.35,1559.25,1.0,1.0,1.0,1.0,1.0,1.0,2.0])     |\n",
      "|1.0  |(19,[1,2,3,5,7,8,9,14],[1.0,24.8,24.8,1.0,1.0,2.0,1.0,1.0])                          |\n",
      "|0.0  |[0.0,59.0,19.3,1192.7,1.0,1.0,1.0,0.0,0.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0]   |\n",
      "|0.0  |(19,[1,2,3,7,8,9,13,14,16,18],[10.0,29.6,299.05,1.0,2.0,1.0,1.0,1.0,1.0,3.0])        |\n",
      "|0.0  |(19,[0,1,2,3,4,10,14,17],[1.0,4.0,75.35,273.4,1.0,1.0,1.0,1.0])                      |\n",
      "|1.0  |(19,[0,1,2,3,4,7,8,9,12,14,15],[1.0,3.0,41.15,132.2,1.0,1.0,2.0,1.0,1.0,1.0,1.0])    |\n",
      "+-----+-------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized_train_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_base = LogisticRegression(labelCol='Churn')\n",
    "lr_trained_model = lr_base.fit(vectorized_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|Churn|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|  125|\n",
      "|  0.0|       1.0|   63|\n",
      "|  1.0|       0.0|   92|\n",
      "|  0.0|       0.0|  535|\n",
      "+-----+----------+-----+\n",
      "\n",
      "F1-Score: 0.8053\n",
      "Accuracy: 0.8098\n",
      "Recall: 0.576\n"
     ]
    }
   ],
   "source": [
    "train_f1, train_accuracy, train_recall, train_conf_mat = evaluate_model(lr_trained_model, vectorized_train_df)\n",
    "train_conf_mat.show()\n",
    "print(f'F1-Score: {round(train_f1, 4)}\\nAccuracy: {round(train_accuracy, 4)}\\nRecall: {round(train_recall, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|Churn|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|   39|\n",
      "|  0.0|       1.0|   28|\n",
      "|  1.0|       0.0|   33|\n",
      "|  0.0|       0.0|  187|\n",
      "+-----+----------+-----+\n",
      "\n",
      "F1-Score: 0.7849\n",
      "Accuracy: 0.7875\n",
      "Recall: 0.5417\n"
     ]
    }
   ],
   "source": [
    "test_f1, test_accuracy, test_recall, test_conf_mat = evaluate_model(lr_trained_model, vectorized_test_df)\n",
    "test_conf_mat.show()\n",
    "print(f'F1-Score: {round(test_f1, 4)}\\nAccuracy: {round(test_accuracy, 4)}\\nRecall: {round(test_recall, 4)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have our first model! Let's take a look at the training dataset metrics.\n",
    "The first thing it tells us is that we have both a F1 score and accuracy of 78%. That's good for a first model.\n",
    "\n",
    "When we look at the confusion matrix and the recall value, we can see that of all the customers that left the company, we could only predict 54%, which means that almost half the customers could churn just beneath our radars. It also tells us that our model learned way more about negative outcomes that about positive outcomes, that could be because of the labels unbalancing on the train data.\n",
    "\n",
    "One thing that we need to point out is **how important the recall metric is** in this case. Since we are trying to predict customers close to leaving the company, a false negative means we couldn't anticipate a churn. So we need to get those false negatives as low as possible. And a higher recall means fewer FN's."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple ways we can deal with an unbalanced dataset, we can oversample it, undersample it, and we can also use cross-validation along with those two options. For now we'll just use undersample, but when we move to the cloud platform we can use a more robust approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Churn|count|\n",
      "+-----+-----+\n",
      "|  0.0|  598|\n",
      "|  1.0|  217|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized_train_df.groupBy('Churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Churn|count|\n",
      "+-----+-----+\n",
      "|  0.0|  225|\n",
      "|  1.0|  217|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "undersampled_0_label = vectorized_train_df.filter('Churn == 0').sample(0.4)\n",
    "undersampled_train_df = undersampled_0_label.union(vectorized_train_df.filter('Churn == 1'))\n",
    "undersampled_train_df.groupBy('Churn').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a perfect solution, but we got a more balanced dataset, let's see how that affects our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_lr_model = lr_base.fit(undersampled_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|Churn|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|   57|\n",
      "|  0.0|       1.0|   68|\n",
      "|  1.0|       0.0|   15|\n",
      "|  0.0|       0.0|  147|\n",
      "+-----+----------+-----+\n",
      "\n",
      "F1-Score: 0.7294\n",
      "Accuracy: 0.7108\n",
      "Recall: 0.7917\n"
     ]
    }
   ],
   "source": [
    "test_f1, test_accuracy, test_recall, test_conf_mat = evaluate_model(undersampled_lr_model, vectorized_test_df)\n",
    "test_conf_mat.show()\n",
    "print(f'F1-Score: {round(test_f1, 4)}\\nAccuracy: {round(test_accuracy, 4)}\\nRecall: {round(test_recall, 4)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got a great improvement on the recall value! We went from predicting 54% of churn cases, to predicting 79%. We also had a little drop in the accuracy and F1 score. That might have happened due to the undersample, which affected the model's ability to predict negative churn cases. Although it's not ideal to have a higher FP rate, in our case, it's better to have a high FP rate, than a high FN rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a model using cross-validation and undersampling and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_validated_model(dataframe, estimator, params, evaluator):\n",
    "    vectorized_data = vectorize_dataframe(dataframe, label='Churn')\n",
    "\n",
    "    cv = CrossValidator(estimator=estimator, evaluator=evaluator, estimatorParamMaps=params,numFolds=3, parallelism=2)\n",
    "    cvModel = cv.fit(vectorized_data)\n",
    "\n",
    "    return cvModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_0_label = numeric_customers_table.filter('Churn == 0').sample(0.4, seed=42)\n",
    "undersampled_df = undersampled_0_label.union(numeric_customers_table.filter('Churn == 1'))\n",
    "params_grid = ParamGridBuilder().addGrid(lr_base.maxIter, [75, 100, 150, 200, 250]).build()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Churn', metricName='recallByLabel', metricLabel=1.0)\n",
    "\n",
    "trained_cvmodel = create_cross_validated_model(undersampled_df, lr_base, params_grid,evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|Churn|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|   57|\n",
      "|  0.0|       1.0|   55|\n",
      "|  1.0|       0.0|   15|\n",
      "|  0.0|       0.0|  160|\n",
      "+-----+----------+-----+\n",
      "\n",
      "F1-Score: 0.7701\n",
      "Accuracy: 0.7561\n",
      "Recall: 0.7917\n"
     ]
    }
   ],
   "source": [
    "test_f1, test_accuracy, test_recall, test_conf_mat = evaluate_model(trained_cvmodel, vectorized_test_df)\n",
    "test_conf_mat.show()\n",
    "print(f'F1-Score: {round(test_f1, 4)}\\nAccuracy: {round(test_accuracy, 4)}\\nRecall: {round(test_recall, 4)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we already got some interesting results! We could see the power of undersampling and cross-validating, and now we can start building our .py script that will be running on our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = 'getMaxIter()'\n",
    "trained_cvmodel.bestModel._java_obj.getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_521bdbf7db6a', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='labelCol', doc='label column name.'): 'Churn',\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='maxIter', doc='max number of iterations (>= 0).'): 75,\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='regParam', doc='regularization parameter (>= 0).'): 0.0,\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5,\n",
       " Param(parent='LogisticRegression_521bdbf7db6a', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_cvmodel.bestModel.extractParamMap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
