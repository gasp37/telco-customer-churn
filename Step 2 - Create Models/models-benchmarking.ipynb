{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(appName='teleco-customer-churn')\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will work more as a guide to help us develop the script that will run on GCP, and since we will run it localy, I'll just grab a sample of around 1000 records, so we can do it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_table = spark.read.csv('../data/WA_Fn-UseC_-Telco-Customer-Churn.csv', header='true', inferSchema='true')\n",
    "customers_table_sample = customers_table.sample(withReplacement=False, fraction=0.15, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1102"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_table_sample.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by applying the same steps we did on the analysis notebook to treat missing values and standardize column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_table_sample = customers_table_sample.withColumnRenamed('gender', 'Gender').withColumnRenamed('tenure', 'Tenure').withColumnRenamed('customerId', 'CustomerId')\n",
    "customers_table_sample = customers_table_sample.replace(subset='TotalCharges', to_replace=' ', value='0.00')\n",
    "customers_table_sample = customers_table_sample.withColumn('TotalCharges', customers_table_sample.TotalCharges.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|CustomerId|Gender|SeniorCitizen|Partner|Dependents|Tenure|PhoneService|   MultipleLines|InternetService|     OnlineSecurity|       OnlineBackup|   DeviceProtection|        TechSupport|        StreamingTV|    StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|6713-OKOMC|Female|            0|     No|        No|    10|          No|No phone service|            DSL|                Yes|                 No|                 No|                 No|                 No|                 No|Month-to-month|              No|        Mailed check|         29.75|       301.9|   No|\n",
      "|8191-XWSZG|Female|            0|     No|        No|    52|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      One year|              No|        Mailed check|         20.65|     1022.95|   No|\n",
      "|4190-MFLUW|Female|            0|    Yes|       Yes|    10|         Yes|              No|            DSL|                 No|                 No|                Yes|                Yes|                 No|                 No|Month-to-month|              No|Credit card (auto...|          55.2|      528.35|  Yes|\n",
      "|7639-LIAYI|  Male|            0|     No|        No|    52|         Yes|             Yes|            DSL|                Yes|                 No|                 No|                Yes|                Yes|                Yes|      Two year|             Yes|Credit card (auto...|         79.75|      4217.8|   No|\n",
      "|8012-SOUDQ|Female|            1|     No|        No|    43|         Yes|             Yes|    Fiber optic|                 No|                Yes|                 No|                 No|                Yes|                 No|Month-to-month|             Yes|    Electronic check|         90.25|     3838.75|   No|\n",
      "|3957-SQXML|Female|            0|    Yes|       Yes|    34|         Yes|             Yes|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      Two year|              No|Credit card (auto...|         24.95|       894.3|   No|\n",
      "|0557-ASKVU|Female|            0|    Yes|       Yes|    18|         Yes|              No|            DSL|                 No|                 No|                Yes|                Yes|                 No|                 No|      One year|             Yes|Credit card (auto...|          54.4|       957.1|   No|\n",
      "|8627-ZYGSZ|  Male|            0|    Yes|        No|    47|         Yes|             Yes|    Fiber optic|                 No|                Yes|                 No|                 No|                 No|                 No|      One year|             Yes|    Electronic check|          78.9|     3650.35|   No|\n",
      "|1891-QRQSA|  Male|            1|    Yes|       Yes|    64|         Yes|             Yes|    Fiber optic|                Yes|                 No|                Yes|                Yes|                Yes|                Yes|      Two year|             Yes|Bank transfer (au...|         111.6|      7099.0|   No|\n",
      "|3887-PBQAO|Female|            0|    Yes|       Yes|    45|         Yes|             Yes|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      One year|             Yes|Credit card (auto...|          25.9|      1216.6|   No|\n",
      "|2796-NNUFI|Female|            0|    Yes|       Yes|    46|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      Two year|             Yes|        Mailed check|         19.95|       927.1|   No|\n",
      "|4767-HZZHQ|  Male|            0|    Yes|       Yes|    30|         Yes|              No|    Fiber optic|                 No|                Yes|                Yes|                 No|                 No|                 No|Month-to-month|              No|Bank transfer (au...|         82.05|      2570.2|   No|\n",
      "|5386-THSLQ|Female|            1|    Yes|        No|    66|          No|No phone service|            DSL|                 No|                Yes|                Yes|                 No|                Yes|                 No|      One year|              No|Bank transfer (au...|         45.55|     3027.25|   No|\n",
      "|6180-YBIQI|  Male|            0|     No|        No|     5|          No|No phone service|            DSL|                 No|                 No|                 No|                 No|                 No|                 No|Month-to-month|              No|        Mailed check|          24.3|       100.2|   No|\n",
      "|6728-DKUCO|Female|            0|    Yes|       Yes|    72|         Yes|             Yes|    Fiber optic|                Yes|                Yes|                 No|                 No|                Yes|                Yes|      One year|             Yes|    Electronic check|        104.15|     7303.05|   No|\n",
      "|2848-YXSMW|  Male|            0|    Yes|       Yes|    72|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      Two year|              No|Credit card (auto...|          19.4|     1363.25|   No|\n",
      "|0404-SWRVG|  Male|            0|     No|        No|     3|         Yes|             Yes|    Fiber optic|                 No|                 No|                 No|                 No|                 No|                 No|Month-to-month|             Yes|    Electronic check|          74.4|      229.55|  Yes|\n",
      "|3930-ZGWVE|  Male|            0|     No|        No|     1|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|Month-to-month|              No|        Mailed check|         19.75|       19.75|   No|\n",
      "|2876-GZYZC|Female|            0|     No|        No|    13|         Yes|             Yes|    Fiber optic|                 No|                 No|                 No|                 No|                 No|                Yes|Month-to-month|             Yes|    Electronic check|         85.95|     1215.65|   No|\n",
      "|6217-KDYWC|  Male|            0|     No|       Yes|    57|         Yes|              No|             No|No internet service|No internet service|No internet service|No internet service|No internet service|No internet service|      Two year|             Yes|        Mailed check|          19.6|     1170.55|   No|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_table_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CustomerId: string, Gender: string, SeniorCitizen: int, Partner: string, Dependents: string, Tenure: int, PhoneService: string, MultipleLines: string, InternetService: string, OnlineSecurity: string, OnlineBackup: string, DeviceProtection: string, TechSupport: string, StreamingTV: string, StreamingMovies: string, Contract: string, PaperlessBilling: string, PaymentMethod: string, MonthlyCharges: double, TotalCharges: double, Churn: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_table_sample.distinct()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "---\n",
    "First, we'll drop de Id column, since it doesn't present any predictive value. Then we'll convert the categorical string variables into numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_table_sample = customers_table_sample.drop('CustomerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+--------------+------------+------+-------+----------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+-----+\n",
      "|SeniorCitizen|Tenure|MonthlyCharges|TotalCharges|Gender|Partner|Dependents|PhoneService|MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|Contract|PaperlessBilling|PaymentMethod|Churn|\n",
      "+-------------+------+--------------+------------+------+-------+----------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+-----+\n",
      "|            0|    10|         29.75|       301.9|   0.0|    0.0|       0.0|         1.0|          2.0|            1.0|           1.0|         0.0|             0.0|        0.0|        1.0|            0.0|     0.0|             1.0|          3.0|  0.0|\n",
      "|            0|    52|         20.65|     1022.95|   0.0|    0.0|       0.0|         0.0|          0.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     2.0|             1.0|          3.0|  0.0|\n",
      "|            0|    10|          55.2|      528.35|   0.0|    1.0|       1.0|         0.0|          0.0|            1.0|           0.0|         0.0|             1.0|        1.0|        1.0|            0.0|     0.0|             1.0|          1.0|  1.0|\n",
      "|            0|    52|         79.75|      4217.8|   1.0|    0.0|       0.0|         0.0|          1.0|            1.0|           1.0|         0.0|             0.0|        1.0|        0.0|            1.0|     1.0|             0.0|          1.0|  0.0|\n",
      "|            1|    43|         90.25|     3838.75|   0.0|    0.0|       0.0|         0.0|          1.0|            0.0|           0.0|         1.0|             0.0|        0.0|        0.0|            0.0|     0.0|             0.0|          0.0|  0.0|\n",
      "|            0|    34|         24.95|       894.3|   0.0|    1.0|       1.0|         0.0|          1.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     1.0|             1.0|          1.0|  0.0|\n",
      "|            0|    18|          54.4|       957.1|   0.0|    1.0|       1.0|         0.0|          0.0|            1.0|           0.0|         0.0|             1.0|        1.0|        1.0|            0.0|     2.0|             0.0|          1.0|  0.0|\n",
      "|            0|    47|          78.9|     3650.35|   1.0|    1.0|       0.0|         0.0|          1.0|            0.0|           0.0|         1.0|             0.0|        0.0|        1.0|            0.0|     2.0|             0.0|          0.0|  0.0|\n",
      "|            1|    64|         111.6|      7099.0|   1.0|    1.0|       1.0|         0.0|          1.0|            0.0|           1.0|         0.0|             1.0|        1.0|        0.0|            1.0|     1.0|             0.0|          2.0|  0.0|\n",
      "|            0|    45|          25.9|      1216.6|   0.0|    1.0|       1.0|         0.0|          1.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     2.0|             0.0|          1.0|  0.0|\n",
      "|            0|    46|         19.95|       927.1|   0.0|    1.0|       1.0|         0.0|          0.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     1.0|             0.0|          3.0|  0.0|\n",
      "|            0|    30|         82.05|      2570.2|   1.0|    1.0|       1.0|         0.0|          0.0|            0.0|           0.0|         1.0|             1.0|        0.0|        1.0|            0.0|     0.0|             1.0|          2.0|  0.0|\n",
      "|            1|    66|         45.55|     3027.25|   0.0|    1.0|       0.0|         1.0|          2.0|            1.0|           0.0|         1.0|             1.0|        0.0|        0.0|            0.0|     2.0|             1.0|          2.0|  0.0|\n",
      "|            0|     5|          24.3|       100.2|   1.0|    0.0|       0.0|         1.0|          2.0|            1.0|           0.0|         0.0|             0.0|        0.0|        1.0|            0.0|     0.0|             1.0|          3.0|  0.0|\n",
      "|            0|    72|        104.15|     7303.05|   0.0|    1.0|       1.0|         0.0|          1.0|            0.0|           1.0|         1.0|             0.0|        0.0|        0.0|            1.0|     2.0|             0.0|          0.0|  0.0|\n",
      "|            0|    72|          19.4|     1363.25|   1.0|    1.0|       1.0|         0.0|          0.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     1.0|             1.0|          1.0|  0.0|\n",
      "|            0|     3|          74.4|      229.55|   1.0|    0.0|       0.0|         0.0|          1.0|            0.0|           0.0|         0.0|             0.0|        0.0|        1.0|            0.0|     0.0|             0.0|          0.0|  1.0|\n",
      "|            0|     1|         19.75|       19.75|   1.0|    0.0|       0.0|         0.0|          0.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     0.0|             1.0|          3.0|  0.0|\n",
      "|            0|    13|         85.95|     1215.65|   0.0|    0.0|       0.0|         0.0|          1.0|            0.0|           0.0|         0.0|             0.0|        0.0|        1.0|            1.0|     0.0|             0.0|          0.0|  0.0|\n",
      "|            0|    57|          19.6|     1170.55|   1.0|    0.0|       1.0|         0.0|          0.0|            2.0|           2.0|         2.0|             2.0|        2.0|        2.0|            2.0|     1.0|             0.0|          3.0|  0.0|\n",
      "+-------------+------+--------------+------------+------+-------+----------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_variables = [variable[0] for variable in customers_table_sample.dtypes if variable[1] == 'string']\n",
    "output_string_variables = [variable+'_numeric' for variable in string_variables]\n",
    "rename_columns_dic = {output_string_variables[index]:string_variables[index] for index in range(len(string_variables))}\n",
    "\n",
    "indexer_model = StringIndexer(inputCols=string_variables, outputCols=output_string_variables)\n",
    "indexer_fitted = indexer_model.fit(customers_table_sample)\n",
    "numeric_customers_table = indexer_fitted.transform(customers_table_sample)\n",
    "\n",
    "numeric_customers_table = numeric_customers_table.drop(*string_variables)\n",
    "numeric_customers_table = numeric_customers_table.withColumnsRenamed(rename_columns_dic)\n",
    "\n",
    "numeric_customers_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SeniorCitizen', 'int'),\n",
       " ('Tenure', 'int'),\n",
       " ('MonthlyCharges', 'double'),\n",
       " ('TotalCharges', 'double'),\n",
       " ('Gender', 'double'),\n",
       " ('Partner', 'double'),\n",
       " ('Dependents', 'double'),\n",
       " ('PhoneService', 'double'),\n",
       " ('MultipleLines', 'double'),\n",
       " ('InternetService', 'double'),\n",
       " ('OnlineSecurity', 'double'),\n",
       " ('OnlineBackup', 'double'),\n",
       " ('DeviceProtection', 'double'),\n",
       " ('TechSupport', 'double'),\n",
       " ('StreamingTV', 'double'),\n",
       " ('StreamingMovies', 'double'),\n",
       " ('Contract', 'double'),\n",
       " ('PaperlessBilling', 'double'),\n",
       " ('PaymentMethod', 'double'),\n",
       " ('Churn', 'double')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_customers_table.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, we got all the variables set as numeric values. We will now create our first model so we can use it as a baseline. I don't expect it to be the most accurate, but after that we can dig more into other pre-processing techniques that will later on improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll just create a few functions that will help us pre-process and evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitter(dataframe, test_ratio = 0.7, seed=42):\n",
    "    pre_split_dataframe = dataframe.withColumn('train_test_index', rand(seed=seed))\n",
    "    \n",
    "    train_dataframe = pre_split_dataframe.filter(pre_split_dataframe.train_test_index <= test_ratio)\n",
    "    test_dataframe = pre_split_dataframe.filter(pre_split_dataframe.train_test_index > test_ratio)\n",
    "\n",
    "    train_dataframe = train_dataframe.drop('train_test_index')\n",
    "    test_dataframe = test_dataframe.drop('train_test_index')\n",
    "\n",
    "    print(f'Rows on train dataframe: {train_dataframe.count()}\\nRows on test dataframe: {test_dataframe.count()}')\n",
    "    return train_dataframe, test_dataframe\n",
    "\n",
    "\n",
    "def vectorize_dataframe(dataframe, label):\n",
    "    features_cols = dataframe.drop(label).columns\n",
    "\n",
    "    vecAssembler = VectorAssembler(inputCols=features_cols, outputCol='features')\n",
    "    vectorized_df = vecAssembler.transform(dataframe)\n",
    "    vectorized_df = vectorized_df.drop(*features_cols)\n",
    "\n",
    "    return vectorized_df\n",
    "\n",
    "def evaluate_model(model, dataframe):\n",
    "    prediction = model.transform(dataframe)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='Churn', metricName='f1', metricLabel=1.0)\n",
    "    f1_score = evaluator.evaluate(prediction)\n",
    "    accuracy_score = evaluator.evaluate(prediction, {evaluator.metricName:'accuracy'})\n",
    "    recall_score = evaluator.evaluate(prediction, {evaluator.metricName:'recallByLabel'})\n",
    "\n",
    "    confusion_matrix = prediction.groupBy('Churn', 'prediction').count()\n",
    "\n",
    "    return f1_score, accuracy_score, recall_score, confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can create our model, we need to do a train-test split. I won't be using RandomSplit() as it presents some unstable results. [You can read about it here](https://sergei-ivanov.medium.com/why-you-should-not-use-randomsplit-in-pyspark-to-split-data-into-train-and-test-58576d539a36). Instead, we'll create a column with random values and filter it, and then we'll vectorize those datasets so we can have them ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows on train dataframe: 815\n",
      "Rows on test dataframe: 287\n"
     ]
    }
   ],
   "source": [
    "first_train_df, first_test_df = train_test_splitter(numeric_customers_table)\n",
    "vectorized_train_df = vectorize_dataframe(first_train_df, label='Churn')\n",
    "vectorized_test_df = vectorize_dataframe(first_test_df, label='Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------+\n",
      "|Churn|features                                                                             |\n",
      "+-----+-------------------------------------------------------------------------------------+\n",
      "|0.0  |(19,[1,2,3,7,8,9,10,14,17,18],[10.0,29.75,301.9,1.0,2.0,1.0,1.0,1.0,1.0,3.0])        |\n",
      "|0.0  |[0.0,52.0,20.65,1022.95,0.0,0.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,1.0,3.0] |\n",
      "|0.0  |(19,[1,2,3,4,8,9,10,13,15,16,18],[52.0,79.75,4217.8,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|0.0  |(19,[0,1,2,3,8,11],[1.0,43.0,90.25,3838.75,1.0,1.0])                                 |\n",
      "|0.0  |[0.0,34.0,24.95,894.3,0.0,1.0,1.0,0.0,1.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0]   |\n",
      "|0.0  |(19,[1,2,3,4,5,8,11,14,16],[47.0,78.9,3650.35,1.0,1.0,1.0,1.0,1.0,2.0])              |\n",
      "|0.0  |[0.0,46.0,19.95,927.1,0.0,1.0,1.0,0.0,0.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,1.0,0.0,3.0]   |\n",
      "|0.0  |(19,[1,2,3,4,5,6,11,12,14,17,18],[30.0,82.05,2570.2,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0])|\n",
      "|0.0  |[1.0,66.0,45.55,3027.25,0.0,1.0,0.0,1.0,2.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,2.0,1.0,2.0] |\n",
      "|1.0  |(19,[1,2,3,4,8,14],[3.0,74.4,229.55,1.0,1.0,1.0])                                    |\n",
      "|0.0  |[0.0,1.0,19.75,19.75,1.0,0.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,0.0,1.0,3.0]    |\n",
      "|0.0  |(19,[1,2,3,8,14,15],[13.0,85.95,1215.65,1.0,1.0,1.0])                                |\n",
      "|1.0  |(19,[1,2,3,8,9,10,13,14,15,17],[8.0,71.15,563.65,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |\n",
      "|1.0  |(19,[1,2,3,5,8,10,14,17],[20.0,82.4,1592.35,1.0,1.0,1.0,1.0,1.0])                    |\n",
      "|0.0  |(19,[1,2,3,5,6,8,11,12,15,18],[15.0,105.35,1559.25,1.0,1.0,1.0,1.0,1.0,1.0,2.0])     |\n",
      "|1.0  |(19,[1,2,3,5,7,8,9,14],[1.0,24.8,24.8,1.0,1.0,2.0,1.0,1.0])                          |\n",
      "|0.0  |[0.0,59.0,19.3,1192.7,1.0,1.0,1.0,0.0,0.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0]   |\n",
      "|0.0  |(19,[1,2,3,7,8,9,13,14,16,18],[10.0,29.6,299.05,1.0,2.0,1.0,1.0,1.0,1.0,3.0])        |\n",
      "|0.0  |(19,[0,1,2,3,4,10,14,17],[1.0,4.0,75.35,273.4,1.0,1.0,1.0,1.0])                      |\n",
      "|1.0  |(19,[0,1,2,3,4,7,8,9,12,14,15],[1.0,3.0,41.15,132.2,1.0,1.0,2.0,1.0,1.0,1.0,1.0])    |\n",
      "+-----+-------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized_train_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_base = LogisticRegression(labelCol='Churn')\n",
    "lr_trained_model = lr_base.fit(vectorized_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|Churn|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|  125|\n",
      "|  0.0|       1.0|   63|\n",
      "|  1.0|       0.0|   92|\n",
      "|  0.0|       0.0|  535|\n",
      "+-----+----------+-----+\n",
      "\n",
      "F1-Score: 0.8053\n",
      "Accuracy: 0.8098\n",
      "Recall: 0.576\n"
     ]
    }
   ],
   "source": [
    "train_f1, train_accuracy, train_recall, train_conf_mat = evaluate_model(lr_trained_model, vectorized_train_df)\n",
    "train_conf_mat.show()\n",
    "print(f'F1-Score: {round(train_f1, 4)}\\nAccuracy: {round(train_accuracy, 4)}\\nRecall: {round(train_recall, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|Churn|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|   39|\n",
      "|  0.0|       1.0|   28|\n",
      "|  1.0|       0.0|   33|\n",
      "|  0.0|       0.0|  187|\n",
      "+-----+----------+-----+\n",
      "\n",
      "F1-Score: 0.7849\n",
      "Accuracy: 0.7875\n",
      "Recall: 0.5417\n"
     ]
    }
   ],
   "source": [
    "test_f1, test_accuracy, test_recall, test_conf_mat = evaluate_model(lr_trained_model, vectorized_test_df)\n",
    "test_conf_mat.show()\n",
    "print(f'F1-Score: {round(test_f1, 4)}\\nAccuracy: {round(test_accuracy, 4)}\\nRecall: {round(test_recall, 4)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have our first model! Let's take a look at the training dataset metrics.\n",
    "The first thing it tells us is that we have both a F1 score and accuracy of 78%. That's good for a first model.\n",
    "\n",
    "When we look at the confusion matrix and the recall value, we can see that of all the customers that left the company, we could only predict 54%, which means that almost half the customers could churn just beneath our radars. It also tells us that our model learned way more about negative outcomes that about positive outcomes, that could be because of the labels unbalancing on the train data.\n",
    "\n",
    "One thing that we need to point out is **how important the recall metric is** in this case. Since we are trying to predict customers close to leaving the company, a false negative means we couldn't anticipate a churn. So we need to get those false negatives as low as possible. And a higher recall means fewer FN's."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple ways we can deal with an unbalanced dataset, we can oversample it, undersample it, and we can also use cross-validation along with those two options. For now we'll just use undersample, but when we move to the cloud platform we can use a more robust approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Churn|count|\n",
      "+-----+-----+\n",
      "|  0.0|  598|\n",
      "|  1.0|  217|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized_train_df.groupBy('Churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Churn|count|\n",
      "+-----+-----+\n",
      "|  0.0|  225|\n",
      "|  1.0|  217|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "undersampled_0_label = vectorized_train_df.filter('Churn == 0').sample(0.4)\n",
    "undersampled_train_df = undersampled_0_label.union(vectorized_train_df.filter('Churn == 1'))\n",
    "undersampled_train_df.groupBy('Churn').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a perfect solution, but we got a more balanced dataset, let's see how that affects our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_lr_model = lr_base.fit(undersampled_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|Churn|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|   57|\n",
      "|  0.0|       1.0|   68|\n",
      "|  1.0|       0.0|   15|\n",
      "|  0.0|       0.0|  147|\n",
      "+-----+----------+-----+\n",
      "\n",
      "F1-Score: 0.7294\n",
      "Accuracy: 0.7108\n",
      "Recall: 0.7917\n"
     ]
    }
   ],
   "source": [
    "test_f1, test_accuracy, test_recall, test_conf_mat = evaluate_model(undersampled_lr_model, vectorized_test_df)\n",
    "test_conf_mat.show()\n",
    "print(f'F1-Score: {round(test_f1, 4)}\\nAccuracy: {round(test_accuracy, 4)}\\nRecall: {round(test_recall, 4)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got a great improvement on the recall value! We went from predicting 54% of churn cases, to predicting 79%. We also had a little drop in the accuracy and F1 score. That might have happened due to the undersample, which affected the model's ability to predict negative churn cases. Although it's not ideal to have a higher FP rate, in our case, it's better to have a high FP rate, than a high FN rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a model using cross-validation and undersampling and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_validated_model(dataframe, estimator, params, evaluator):\n",
    "    vectorized_data = vectorize_dataframe(dataframe, label='Churn')\n",
    "\n",
    "    cv = CrossValidator(estimator=estimator, evaluator=evaluator, estimatorParamMaps=params,numFolds=3, parallelism=2)\n",
    "    cvModel = cv.fit(vectorized_data)\n",
    "\n",
    "    return cvModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_0_label = numeric_customers_table.filter('Churn == 0').sample(0.4, seed=42)\n",
    "undersampled_df = undersampled_0_label.union(numeric_customers_table.filter('Churn == 1'))\n",
    "params_grid = ParamGridBuilder().addGrid(lr_base.maxIter, [75, 100, 150, 200, 250]).build()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Churn', metricName='recallByLabel', metricLabel=1.0)\n",
    "\n",
    "trained_cvmodel = create_cross_validated_model(undersampled_df, lr_base, params_grid,evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|Churn|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|   57|\n",
      "|  0.0|       1.0|   55|\n",
      "|  1.0|       0.0|   15|\n",
      "|  0.0|       0.0|  160|\n",
      "+-----+----------+-----+\n",
      "\n",
      "F1-Score: 0.7701\n",
      "Accuracy: 0.7561\n",
      "Recall: 0.7917\n"
     ]
    }
   ],
   "source": [
    "test_f1, test_accuracy, test_recall, test_conf_mat = evaluate_model(trained_cvmodel, vectorized_test_df)\n",
    "test_conf_mat.show()\n",
    "print(f'F1-Score: {round(test_f1, 4)}\\nAccuracy: {round(test_accuracy, 4)}\\nRecall: {round(test_recall, 4)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we already got some interesting results! We could see the power of undersampling and cross-validating, and now we can start building our .py script that will be running on our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-a0899eb283ba>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-33-a0899eb283ba>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    trained_cvmodel.bestModel._java_obj.(get+param+'()')\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "param = 'getMaxIter()'\n",
    "trained_cvmodel.bestModel._java_obj."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
